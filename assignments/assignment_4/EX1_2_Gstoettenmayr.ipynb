{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a7e5ba",
   "metadata": {},
   "source": [
    "# Assignment 4 - Jonas Gstöttenmayr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b922f",
   "metadata": {},
   "source": [
    "## 1) Ingesting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669e8035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No logs directory found\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from itertools import product\n",
    "from utils import train_model\n",
    "\n",
    "if os.path.exists(\"logs\"):\n",
    "    shutil.rmtree(\"logs\")\n",
    "    print(\"Logs directory cleared\")\n",
    "else:\n",
    "    print(\"No logs directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac50d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 64), y_train: (60000,)\n",
      "X_test: (10000, 64), y_test: (10000,)\n",
      "Classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import keras\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "sym_dim=8\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0 #this is how we normally scale with images\n",
    "X_test = X_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0 #same for test\n",
    "\n",
    "\n",
    "X_train = resize(X_train.reshape(-1, 28, 28), (len(X_train), sym_dim, sym_dim)).reshape(-1, sym_dim*sym_dim).astype(\"float32\")\n",
    "X_test = resize(X_test.reshape(-1, 28, 28), (len(X_test), sym_dim, sym_dim)).reshape(-1, sym_dim*sym_dim).astype(\"float32\")\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3913042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650941e",
   "metadata": {},
   "source": [
    "## Exercise 1: Extended Random Search (7 points)\n",
    "\n",
    "### Part a) Extending the Hyperparameter Search (6 points)\n",
    "\n",
    "Based on the notebook `mnist_fashion_fcnn_simple.ipynb` from exercise 9, extend the hyperparameter search with additional parameters and implement a systematic search strategy. **Feel free to adapt/add/delete code in any way you see fit.**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Extend the search space** with at least 3 additional hyperparameters beyond `hidden_layers` and `dropout_rate`.\n",
    "\n",
    "2. **Modify the necessary functions:**\n",
    "\n",
    "   - Update `create_hyperparams()` to generate configurations for your new parameters\n",
    "   - Adapt `create_fcnn()` to accept and use these parameters\n",
    "   - Modify `run_search()` to pass the parameters correctly\n",
    "\n",
    "3. **Run at least 100 different configurations** and document your findings.\n",
    "\n",
    "4. **Explain your choices:** For each hyperparameter you add, explicitly describe:\n",
    "   - Why you chose this parameter\n",
    "   - What range/values you selected and why\n",
    "   - What impact you expect it to have\n",
    "\n",
    "**Note:** You will need to modify multiple functions, but you can base everything on the existing `mnist_fashion_fcnn_simple.ipynb` notebook structure.\n",
    "\n",
    "### Part b) Fixing the Data Leak (1 point)\n",
    "\n",
    "There is a subtle data leak in the current search implementation.\n",
    "\n",
    "**Hint:** Look carefully at how we perform validation in the `train_model()` function in `utils.py`. Consider what data the model sees during training and how this might affect the fairness of comparing different random seeds or configurations.\n",
    "\n",
    "Identify the issue, explain what's wrong, and fix it.\n",
    "\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bda1c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb2c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac940cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fcnn(\n",
    "    input_dim: int,\n",
    "    num_classes: int,\n",
    "    hidden_layers: list[int] = [512, 256, 128],\n",
    "    dropout_rate: float = 0.3,\n",
    "    name: str = \"fcnn\",\n",
    "    learning_rate: float = 2e-4,\n",
    "    activation_function: str = \"relu\",\n",
    "    optimizer: opt.Optimizer = opt.Adam,\n",
    ") -> keras.Model:\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    \n",
    "    for _, units in enumerate(hidden_layers):\n",
    "        x = keras.layers.Dense(units)(x)\n",
    "        x = keras.layers.Activation(activation_function)(x)\n",
    "        if dropout_rate > 0:\n",
    "            x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    model.compile(\n",
    "        optimizer=optimizer(learning_rate=learning_rate),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac60889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765469683.175884   30171 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14793 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fashion_fcnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"fashion_fcnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,280</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │          \u001b[38;5;34m33,280\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">198,794</span> (776.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m198,794\u001b[0m (776.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">198,794</span> (776.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m198,794\u001b[0m (776.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing to see if the creat fun works\n",
    "basic_model = create_fcnn(\n",
    "    input_dim=sym_dim*sym_dim,\n",
    "    num_classes=10,\n",
    "    optimizer=keras.optimizers.Adam,\n",
    "    hidden_layers=[512, 256, 128],\n",
    "    dropout_rate=0.3,\n",
    "    name=\"fashion_fcnn\"\n",
    ")\n",
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a646094",
   "metadata": {},
   "source": [
    "### b) Data Leak\n",
    "\n",
    "What was wrong?\n",
    "\n",
    "We trained on the whole training set and than evaluated on the first 10000 items of the dataset, so we were using data the model had trained on to evaluate it a clear data leak! \n",
    "\n",
    "I fixed this by simply training on only the first 50,000 images, than evaluationg on the next 10,000. The test set is reserved for the best model at we choose at the end to see it's actuall estimated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac659ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_hyperparams(\n",
    "    n: int,\n",
    "    num_layers_range: list[int] = [1, 3],\n",
    "    units_choices: list[int] = [128, 256, 512],\n",
    "    dropout_range: tuple[float, float] = (0.1, 0.5),\n",
    "    lr: tuple[float, float] = (5e-4, 25e-4), # new\n",
    "    activation_function: list[str] = [\"relu\"], # new\n",
    "    optimizers: list[keras.optimizers.Optimizer] = [opt.Adam]\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    params_list = []\n",
    "    i = 0\n",
    "    # making sure it uses different configurations, by going over all unique ones\n",
    "    for mix in product(num_layers_range, units_choices, activation_function, optimizers):\n",
    "        params_list.append({\n",
    "            \"hidden_layers\": [mix[1] for _ in range(mix[0])],\n",
    "            \"dropout_rate\": round(rng.uniform(*dropout_range), 3),\n",
    "            \"learning_rate\": rng.uniform(*lr),\n",
    "            \"activation_function\": mix[2],\n",
    "            \"optimzer\": mix[3]\n",
    "        })\n",
    "        i+= 1\n",
    "        if i == n:\n",
    "            break\n",
    "    return pd.DataFrame(params_list)\n",
    "\n",
    "\n",
    "def run_search(\n",
    "    hp_df: pd.DataFrame,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    epochs: int = 10,\n",
    ") -> tuple[keras.Model, dict, pd.DataFrame]:\n",
    "    results = hp_df.copy()\n",
    "    results[\"val_acc\"] = -1.0\n",
    "    models = {}\n",
    "    \n",
    "    for i, row in hp_df.iterrows():\n",
    "        params = {\n",
    "            \"hidden_layers\": row[\"hidden_layers\"],\n",
    "            \"dropout_rate\": row[\"dropout_rate\"],\n",
    "            \"learning_rate\": row[\"learning_rate\"],\n",
    "            \"activation_function\": row[\"activation_function\"],\n",
    "            \"optimizer\": row[\"optimzer\"],\n",
    "        }\n",
    "        \n",
    "        model = create_fcnn(input_dim=X_train.shape[1], num_classes=10, name=f\"trial_{i}\", **params)\n",
    "        \n",
    "        layers_str = \"_\".join(map(str, params[\"hidden_layers\"]))\n",
    "        model_name = f\"rs_L{layers_str}_D{int(params['dropout_rate'])}\"\n",
    "        \n",
    "        model, _ = train_model(model, X_train[:50000], y_train[:50000], model_name=model_name, epochs=epochs, early_stopping=True, verbose=0)\n",
    "        _, val_acc = model.evaluate(X_train[50000:], y_train[50000:], verbose=0)\n",
    "        \n",
    "        results.loc[i, \"val_acc\"] = round(val_acc, 4)\n",
    "        models[i] = model\n",
    "        \n",
    "        print(f\"Trial {i+1}/{len(hp_df)}: acc={val_acc:.4f}\")\n",
    "    \n",
    "    results = results.sort_values(\"val_acc\", ascending=False).reset_index(drop=True)\n",
    "    best_idx = results.iloc[0].name if \"name\" in dir(results.iloc[0]) else 0\n",
    "    \n",
    "    print(f\"\\n✓ Best: val_acc={results.iloc[0]['val_acc']:.4f}\")\n",
    "    return models[hp_df.index[results.index[0]]], results.iloc[0].to_dict(), results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58191e",
   "metadata": {},
   "source": [
    "#### 4. The hyperparameters\n",
    "\n",
    "I added:\n",
    "\n",
    "- **learning rate** (lr)\n",
    "    How large a step we make each adjustment of the weights\n",
    "\n",
    "    My choise comes from the default keras learning rate of 1e-3 or 0.0001, I decided to randomly choose values in a range of 0.00005-0.00025, so between half and 2.5 times the original rate, as it would deviate by a reasonable amount but not too much\n",
    "\n",
    "    This parameter also interacts with the optimizers.\n",
    "    \n",
    "    Expected impact: with low eochs I hope a higher learning rate increase the accuracy.\n",
    "- **activation function**\n",
    "    Which function the hidden layers use for activation and backprobagation\n",
    "\n",
    "    Why? Honest answer: I just wanted to use all elu variants\\\n",
    "    Techincal answer: Altough we don't have to deep a network so even sigmoid could work it is still interesting which, and if how the activation function effects the model. Especially with the non-scalled data and the small number of epochs, the way the activation function makes our deltas which are backprobagated could/will effect how much we can adjust our weights from the initial value to fit the data.\n",
    "\n",
    "    All the functions are chosen for beeing part of the elu familiy, allowing them to train deep models like relue.\\\n",
    "    Very sort overview:\n",
    "\n",
    "    celu: relu, but continously differentiable\\\n",
    "    elu: relu, but continously differentiable and it does not have 0 for no activation but negative numbers\\\n",
    "    gelu: weigh inputs by their value, unlike relu which gates by sign, the activation is x multiplied by the cumulative probability of x in standard normal distribution\\\n",
    "    relu: good old reliable\\\n",
    "    selu: elu but it has fixed parameters that scale it\\\n",
    "    silu: ~~I missread it as selu before~~, or swish is is unbounded above 0 but bouned below, it multiplied x with the sigmoid of x, so if the sigmoid is close to 0 it will have a almost 0 activation\n",
    "\n",
    "    Expected impact: Little to none, maybe with our smaller epcochs certain ones can help learn faster by modifying the weights more.\n",
    "- **Optimizer**\n",
    "    Which optimizer we use, the optimizer calculates the update step using the learning rate and calculated error.\n",
    "\n",
    "    Optimizers differ in how they calculate the step size for the weights, thus they require different batch-sizes and learning rates.\n",
    "\n",
    "    With it I want to see which optimizer is good for my range of learning rates and if I can get way with using the more memory efficient lion over Adam (important for large training sets which require heaps of memory)\n",
    "\n",
    "    My options:\n",
    "    Adam: the classical choise, Bodenhofer approved -> uses stochasitc gradient descent based on first and second-order\\\n",
    "    Lion: also stochastic gradient descent, but unlike adam it usees the sign operator to control the magnitude of updates, it is more memory efficient than adam as no second-order derivative is used.\n",
    "\n",
    "    Expected impact: Rather small as both are quite good and memory is not an issue, at most one will be slightly better with the current learning rate.\n",
    "\n",
    "Was there:\n",
    "- the number of layers: I choose a lower number to not have to wait forever to finish training\n",
    "- units_choises: how many neurons a hidden layer has, once more i choose lower numbers to reduce training time\n",
    "- dropout range: slightly below the recommended range of 0-0.5, 0.5 drops to many and won't be too good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37572c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>optimzer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>celu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>celu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[32, 32]</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>selu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>selu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>silu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>silu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hidden_layers  dropout_rate  learning_rate activation_function  \\\n",
       "0            [32, 32]         0.310       0.001378                 elu   \n",
       "1            [32, 32]         0.343       0.001895                 elu   \n",
       "2            [32, 32]         0.038       0.002451                celu   \n",
       "3            [32, 32]         0.304       0.002072                celu   \n",
       "4            [32, 32]         0.051       0.001401                gelu   \n",
       "..                ...           ...            ...                 ...   \n",
       "103  [64, 64, 64, 64]         0.053       0.001855                relu   \n",
       "104  [64, 64, 64, 64]         0.049       0.001513                selu   \n",
       "105  [64, 64, 64, 64]         0.278       0.001662                selu   \n",
       "106  [64, 64, 64, 64]         0.080       0.002108                silu   \n",
       "107  [64, 64, 64, 64]         0.286       0.001978                silu   \n",
       "\n",
       "                                     optimzer  \n",
       "0    <class 'keras.src.optimizers.adam.Adam'>  \n",
       "1    <class 'keras.src.optimizers.lion.Lion'>  \n",
       "2    <class 'keras.src.optimizers.adam.Adam'>  \n",
       "3    <class 'keras.src.optimizers.lion.Lion'>  \n",
       "4    <class 'keras.src.optimizers.adam.Adam'>  \n",
       "..                                        ...  \n",
       "103  <class 'keras.src.optimizers.lion.Lion'>  \n",
       "104  <class 'keras.src.optimizers.adam.Adam'>  \n",
       "105  <class 'keras.src.optimizers.lion.Lion'>  \n",
       "106  <class 'keras.src.optimizers.adam.Adam'>  \n",
       "107  <class 'keras.src.optimizers.lion.Lion'>  \n",
       "\n",
       "[108 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_df = create_hyperparams(\n",
    "    n=128,\n",
    "    num_layers_range= [2,3,4],\n",
    "    units_choices=[32, 48, 64],\n",
    "    dropout_range=(0.0, 0.4),\n",
    "    lr = (5e-4, 25e-4),\n",
    "    activation_function = [\"elu\", \"celu\", \"gelu\", \"relu\", \"selu\", \"silu\"],\n",
    "    optimizers=[opt.Adam, opt.Lion]\n",
    ")\n",
    "hp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "results_df_path = \"/home/azureuser/cloudfiles/code/Users/s2410929009/NDL3Repo/assignments/assignment_4/exports/results.parquet\"\n",
    "best_model_path = \"/home/azureuser/cloudfiles/code/Users/s2410929009/NDL3Repo/assignments/assignment_4/exports/best_mode.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f427ea",
   "metadata": {},
   "source": [
    "The message: \"Restoring model weights from the end of the best epoch: x.\"\n",
    "does not mean weights are beeing transferd, but that early stopping is doing it's job, so no impact on grid search, simply kears choosing the best fit for it\n",
    "\n",
    "Took roughly 45 min to train all 108 configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 1/108: acc=0.8000\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 2/108: acc=0.8202\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 3/108: acc=0.8453\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Trial 4/108: acc=0.8204\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 5/108: acc=0.8433\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 6/108: acc=0.8216\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 7/108: acc=0.8229\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 8/108: acc=0.8183\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 9/108: acc=0.8001\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 10/108: acc=0.8103\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 11/108: acc=0.8123\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 12/108: acc=0.8117\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 13/108: acc=0.8088\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 14/108: acc=0.8412\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 15/108: acc=0.8455\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 16/108: acc=0.8139\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 17/108: acc=0.8435\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 18/108: acc=0.8504\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 19/108: acc=0.8497\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Trial 20/108: acc=0.8185\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 21/108: acc=0.8242\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 22/108: acc=0.8308\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 23/108: acc=0.8276\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 24/108: acc=0.8508\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 25/108: acc=0.8189\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 26/108: acc=0.8585\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 27/108: acc=0.8169\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 28/108: acc=0.8145\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 29/108: acc=0.8463\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 30/108: acc=0.8576\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 31/108: acc=0.8452\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 32/108: acc=0.7979\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 33/108: acc=0.8195\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 34/108: acc=0.8356\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 35/108: acc=0.8464\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Trial 36/108: acc=0.8530\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 37/108: acc=0.7892\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 38/108: acc=0.8464\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 39/108: acc=0.8304\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Trial 40/108: acc=0.8075\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 41/108: acc=0.8198\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 42/108: acc=0.8292\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 43/108: acc=0.8436\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Trial 44/108: acc=0.7625\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 45/108: acc=0.8263\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 46/108: acc=0.8266\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 47/108: acc=0.8255\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 48/108: acc=0.8387\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 49/108: acc=0.8268\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Trial 50/108: acc=0.8175\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 51/108: acc=0.8099\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Trial 52/108: acc=0.8176\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 53/108: acc=0.8326\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 54/108: acc=0.8375\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 55/108: acc=0.8521\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 56/108: acc=0.8147\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 57/108: acc=0.8379\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 58/108: acc=0.8156\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 59/108: acc=0.8296\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Trial 60/108: acc=0.8301\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 61/108: acc=0.8361\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 62/108: acc=0.8334\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 63/108: acc=0.8478\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 64/108: acc=0.8511\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 65/108: acc=0.8568\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Trial 66/108: acc=0.8177\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 67/108: acc=0.8434\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 68/108: acc=0.8223\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 69/108: acc=0.8036\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 70/108: acc=0.8175\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 71/108: acc=0.8393\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Trial 72/108: acc=0.7905\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 73/108: acc=0.8097\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 74/108: acc=0.8272\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 75/108: acc=0.8037\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 76/108: acc=0.7211\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 77/108: acc=0.8392\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Trial 78/108: acc=0.8275\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 79/108: acc=0.8348\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 80/108: acc=0.6524\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Trial 81/108: acc=0.8455\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 82/108: acc=0.8407\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 83/108: acc=0.8050\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Trial 84/108: acc=0.7744\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 85/108: acc=0.8377\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Trial 86/108: acc=0.8120\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Trial 87/108: acc=0.8377\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Trial 88/108: acc=0.8485\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 89/108: acc=0.8414\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 90/108: acc=0.7124\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 91/108: acc=0.8020\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 92/108: acc=0.7672\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 93/108: acc=0.8086\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 94/108: acc=0.8325\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 95/108: acc=0.8119\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 96/108: acc=0.8181\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 97/108: acc=0.8597\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Trial 98/108: acc=0.8341\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 99/108: acc=0.8316\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Trial 100/108: acc=0.8189\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 101/108: acc=0.8325\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 102/108: acc=0.8441\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 103/108: acc=0.8560\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 104/108: acc=0.7854\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Trial 105/108: acc=0.8499\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 106/108: acc=0.7245\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Trial 107/108: acc=0.8494\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Trial 108/108: acc=0.7757\n",
      "\n",
      "✓ Best: val_acc=0.8597\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>optimzer</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>selu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[32, 32, 32, 32]</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>celu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[48, 48, 48, 48]</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[32, 32, 32, 32]</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.6524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hidden_layers  dropout_rate  learning_rate activation_function  \\\n",
       "0    [64, 64, 64, 64]         0.049       0.002162                 elu   \n",
       "1            [64, 64]         0.080       0.000515                 elu   \n",
       "2            [64, 64]         0.056       0.000729                gelu   \n",
       "3        [64, 64, 64]         0.041       0.001675                gelu   \n",
       "4    [64, 64, 64, 64]         0.006       0.000959                relu   \n",
       "..                ...           ...            ...                 ...   \n",
       "103      [32, 32, 32]         0.289       0.001424                relu   \n",
       "104  [64, 64, 64, 64]         0.278       0.001662                selu   \n",
       "105  [32, 32, 32, 32]         0.331       0.002292                celu   \n",
       "106  [48, 48, 48, 48]         0.357       0.001997                gelu   \n",
       "107  [32, 32, 32, 32]         0.291       0.002037                relu   \n",
       "\n",
       "                                     optimzer  val_acc  \n",
       "0    <class 'keras.src.optimizers.adam.Adam'>   0.8597  \n",
       "1    <class 'keras.src.optimizers.lion.Lion'>   0.8585  \n",
       "2    <class 'keras.src.optimizers.lion.Lion'>   0.8576  \n",
       "3    <class 'keras.src.optimizers.adam.Adam'>   0.8568  \n",
       "4    <class 'keras.src.optimizers.adam.Adam'>   0.8560  \n",
       "..                                        ...      ...  \n",
       "103  <class 'keras.src.optimizers.lion.Lion'>   0.7625  \n",
       "104  <class 'keras.src.optimizers.lion.Lion'>   0.7245  \n",
       "105  <class 'keras.src.optimizers.lion.Lion'>   0.7211  \n",
       "106  <class 'keras.src.optimizers.lion.Lion'>   0.7124  \n",
       "107  <class 'keras.src.optimizers.lion.Lion'>   0.6524  \n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Path(results_df_path).exists() and Path(best_model_path).exists:\n",
    "    best_model = keras.saving.load_model()\n",
    "    results_df = pd.read_parquet(results_df_path)\n",
    "    best_params = results_df.iloc[0].to_dict()\n",
    "else:\n",
    "    best_model, best_params, results_df = run_search(hp_df, X_train, y_train, epochs=10) # changed to fewer epochs to safe on compute time\n",
    "    # safing the results in case of loosing the state, don't want to retrain for another 40 min\n",
    "    copy = results_df\n",
    "    copy[\"optimzer\"] = copy[\"optimzer\"].astype(str)\n",
    "    copy.to_parquet(results_df_path) \n",
    "    best_model.save(best_model_path)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fccce91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>optimzer</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>silu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[48, 48, 48]</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.8521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>celu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[48, 48]</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>silu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[48, 48]</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.8504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hidden_layers  dropout_rate  learning_rate activation_function  \\\n",
       "0  [64, 64, 64, 64]         0.049       0.002162                 elu   \n",
       "1          [64, 64]         0.080       0.000515                 elu   \n",
       "2          [64, 64]         0.056       0.000729                gelu   \n",
       "3      [64, 64, 64]         0.041       0.001675                gelu   \n",
       "4  [64, 64, 64, 64]         0.006       0.000959                relu   \n",
       "5          [64, 64]         0.086       0.001317                silu   \n",
       "6      [48, 48, 48]         0.039       0.002305                relu   \n",
       "7      [64, 64, 64]         0.132       0.000789                celu   \n",
       "8          [48, 48]         0.155       0.001077                silu   \n",
       "9          [48, 48]         0.188       0.000879                gelu   \n",
       "\n",
       "                                   optimzer  val_acc  \n",
       "0  <class 'keras.src.optimizers.adam.Adam'>   0.8597  \n",
       "1  <class 'keras.src.optimizers.lion.Lion'>   0.8585  \n",
       "2  <class 'keras.src.optimizers.lion.Lion'>   0.8576  \n",
       "3  <class 'keras.src.optimizers.adam.Adam'>   0.8568  \n",
       "4  <class 'keras.src.optimizers.adam.Adam'>   0.8560  \n",
       "5  <class 'keras.src.optimizers.lion.Lion'>   0.8530  \n",
       "6  <class 'keras.src.optimizers.adam.Adam'>   0.8521  \n",
       "7  <class 'keras.src.optimizers.lion.Lion'>   0.8511  \n",
       "8  <class 'keras.src.optimizers.lion.Lion'>   0.8508  \n",
       "9  <class 'keras.src.optimizers.lion.Lion'>   0.8504  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5fbced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>activation_function</th>\n",
       "      <th>optimzer</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>elu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.adam.Adam'&gt;</td>\n",
       "      <td>0.7892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>silu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[32, 32, 32, 32]</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>silu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>[48, 48, 48, 48]</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>[32, 32, 32]</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[64, 64, 64, 64]</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>selu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[32, 32, 32, 32]</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>celu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[48, 48, 48, 48]</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>gelu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.7124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[32, 32, 32, 32]</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>relu</td>\n",
       "      <td>&lt;class 'keras.src.optimizers.lion.Lion'&gt;</td>\n",
       "      <td>0.6524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hidden_layers  dropout_rate  learning_rate activation_function  \\\n",
       "98       [32, 32, 32]         0.341       0.000968                 elu   \n",
       "99   [64, 64, 64, 64]         0.053       0.001855                relu   \n",
       "100  [64, 64, 64, 64]         0.286       0.001978                silu   \n",
       "101  [32, 32, 32, 32]         0.332       0.002117                silu   \n",
       "102  [48, 48, 48, 48]         0.208       0.001132                relu   \n",
       "103      [32, 32, 32]         0.289       0.001424                relu   \n",
       "104  [64, 64, 64, 64]         0.278       0.001662                selu   \n",
       "105  [32, 32, 32, 32]         0.331       0.002292                celu   \n",
       "106  [48, 48, 48, 48]         0.357       0.001997                gelu   \n",
       "107  [32, 32, 32, 32]         0.291       0.002037                relu   \n",
       "\n",
       "                                     optimzer  val_acc  \n",
       "98   <class 'keras.src.optimizers.adam.Adam'>   0.7892  \n",
       "99   <class 'keras.src.optimizers.lion.Lion'>   0.7854  \n",
       "100  <class 'keras.src.optimizers.lion.Lion'>   0.7757  \n",
       "101  <class 'keras.src.optimizers.lion.Lion'>   0.7744  \n",
       "102  <class 'keras.src.optimizers.lion.Lion'>   0.7672  \n",
       "103  <class 'keras.src.optimizers.lion.Lion'>   0.7625  \n",
       "104  <class 'keras.src.optimizers.lion.Lion'>   0.7245  \n",
       "105  <class 'keras.src.optimizers.lion.Lion'>   0.7211  \n",
       "106  <class 'keras.src.optimizers.lion.Lion'>   0.7124  \n",
       "107  <class 'keras.src.optimizers.lion.Lion'>   0.6524  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fdb7582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      "hidden_layers [64, 64, 64, 64]\n",
      "dropout_rate 0.049\n",
      "learning_rate 0.002162225344249812\n",
      "activation_function elu\n",
      "optimzer <class 'keras.src.optimizers.adam.Adam'>\n",
      "val_acc 0.8597\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \")\n",
    "for k, v in best_params.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31efba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.7500 - loss: 0.6479"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8016 - loss: 0.5468\n",
      "For the best model the accuracy on the test set is: 79.74%\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = best_model.evaluate(X_test, y_test)\n",
    "print(f\"For the best model the accuracy on the test set is: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34246d7d",
   "metadata": {},
   "source": [
    "### Result:\n",
    "\n",
    "The bes model is the one with the most neurons per layer and most layers, indicating that we are most likely underfitting, so either more neurons or more epcohs (very likley) are needed.\n",
    "\n",
    "Otherwise as mentioned above Lion is quite good with small learning rates but VERY bad with larger learning rates (the 9 worst models are with lion optimization and large learning rates) so adam is more resistant to the learning rate (be it small or large), but if the learning rate is chosen well Lion can match adam as it takes up quite a few spots in the top 10.\n",
    "\n",
    "Generally lower dropout rates are favoured (most top 10 have smaller dropout rates), which makes sense since we are most likly underfitting and dropout is to prevent overfitting.\n",
    "\n",
    "The activation functions are very diverse, altough it seems that relue specificly leans more towards the bad side of the models.\n",
    "\n",
    "Rather interesting is that a large model canno't safe bad hyperparameters as a model the same size as the best one is among the worst 5 models just through it's large dropout rate + large learningrate/lion combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863300f",
   "metadata": {},
   "source": [
    "## Exercise 2: Keras Tuner (4 points)\n",
    "\n",
    "Explore systematic hyperparameter optimization using Keras Tuner. The principle behind it is quite similar to the implementation we did by hand, however it has support for other search algorithms than random search. Documentation can be found under: https://keras.io/keras_tuner/.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. **Choose and explain one optimization strategy:**\n",
    "\n",
    "   - Either Hyperband Optimization\n",
    "   - Or Bayesian Optimization\n",
    "\n",
    "   Briefly describe how the chosen method works (search for an appropriate reference paper or other academic resource! Hint: look at what keras-tuner cites) and compare to random search.\n",
    "\n",
    "2. **Implement the search:**\n",
    "\n",
    "   - Use Keras Tuner with your chosen strategy on the Fashion MNIST dataset\n",
    "   - Build a small comparison experiment with random search from exercise 1. (e.g. convergence speed.)\n",
    "   - Is such an approach inherently better than random search with additional manual tuning? Reason in one sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfb345",
   "metadata": {},
   "source": [
    "### 1) Explaining Hyperband optimization\n",
    "\n",
    "https://jmlr.org/papers/volume18/16-558/16-558.pdf\n",
    "\n",
    "It is a newer (2018) highly efficient optimization method, that tries to avoid the problems of the older (2012) baysian optimizaion.\n",
    "\n",
    "The hyperband algorithm is an extension of the SuccessiveHalving (SH) Algorithm, which as it names suggests halves the amount of hyperparameter configurations it considers every \"round\". The SH algorithm uniformly allocates a budget to each set of hyperparameters before evaluating them and throwing out the worse half. As the set of parameters get ruduced the freed up budged can be used on the better half.\n",
    "\n",
    "The way it exstends upon the SH Algorithm is by doing an easentially grid-sear over feasible values of how many reasources each set should be able to use from the getgo. It does this by running mulitple instances of SH in parrallel in the beginning with different resource allogations to make sure not to discard good solutions to early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae58a0c",
   "metadata": {},
   "source": [
    "<!-- ## 2) Implementing Search -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feea26a",
   "metadata": {},
   "source": [
    "## 2 Implementing Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15a27d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b14a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 34s]\n",
      "val_accuracy: 0.8299000263214111\n",
      "\n",
      "Best val_accuracy So Far: 0.8632000088691711\n",
      "Total elapsed time: 00h 19m 24s\n",
      "CPU times: user 19min 55s, sys: 1min 54s, total: 21min 50s\n",
      "Wall time: 17min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Functional name=hypermodel, built=True>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "def hypermodel(hp):\n",
    "    # It's good practice to add an Input layer or define the input_shape \n",
    "    # in the first Dense layer. Assuming input shape is 784 for flat images.\n",
    "    \n",
    "    # starts here\n",
    "    inputs = keras.layers.Input(shape=(sym_dim*sym_dim,))\n",
    "    x = inputs\n",
    "    \n",
    "    units = hp.Choice('units_1', [32, 48, 64])\n",
    "    activation_function = hp.Choice(\"activation_1\", [\"elu\", \"celu\", \"gelu\", \"relu\", \"selu\", \"silu\"])\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.4, sampling='linear')\n",
    "    for i in range(hp.Choice(\"layers\", [2,3,4])):\n",
    "        x = keras.layers.Dense(units)(x)\n",
    "        x = keras.layers.Activation(activation_function)(x)\n",
    "        if dropout_rate > 0:\n",
    "            x = keras.layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'lion'])\n",
    "    learning_rate = hp.Float('learning_rate', min_value=5e-4, max_value=25e-4, sampling='linear')\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = opt.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = opt.Lion(learning_rate=learning_rate)\n",
    "    \n",
    "    outputs = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"hypermodel\")\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    hypermodel=hypermodel,\n",
    "    objective=\"val_accuracy\", # monitor validation metric, not training loss\n",
    "    max_epochs=10,\n",
    "    hyperband_iterations=2,\n",
    "    seed=42,\n",
    "    project_name=\"hyperband\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train[:50_000], y_train[:50_000], epochs=10, validation_data=(X_train[50_000:], y_train[50_000:]))\n",
    "best_tuned = tuner.get_best_models()[0]\n",
    "# hypermodel(keras_tuner.HyperParameters())\n",
    "best_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a539e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units_1': 32,\n",
       " 'activation_1': 'celu',\n",
       " 'learning_rate': 0.0006040691120683395,\n",
       " 'layers': 4,\n",
       " 'optimizer': 'lion',\n",
       " 'tuner/epochs': 10,\n",
       " 'tuner/initial_epoch': 0,\n",
       " 'tuner/bracket': 0,\n",
       " 'tuner/round': 0}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters(num_trials=1)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e13a0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m307/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8560 - loss: 0.3996"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.3996\n",
      "For the best model the accuracy on the test set is: 85.53%\n"
     ]
    }
   ],
   "source": [
    "_, test_acc = best_tuned.evaluate(X_test, y_test)\n",
    "print(f\"For the best model the accuracy on the test set is: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "df40bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tuned_path = \"/home/azureuser/cloudfiles/code/Users/s2410929009/NDL3Repo/assignments/assignment_4/exports/best_tuned.keras\"\n",
    "\n",
    "best_model.save(best_tuned_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94904179",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "This is quite the surpirse for me, as my approach for random search was more grid-search as I tried most options.\n",
    "\n",
    "But what this method has over the ranodom search is the ability to test all the different params in the ranges for dropout and learning rate.\n",
    "\n",
    "So while a surprise it is quite logical for it to have better training accuracy than the random search, what is more surprising than that though is that the test accuracy is almost as high as the validation accuracy, so the hyperband optimizer was able to find a not only a objectivly better solution (with less complexity to boot).\n",
    "\n",
    "As such I would recommend the Hyperband algorithm for parameter tuning and will be using it again in the future.\n",
    "\n",
    "Personally the most intersting part of this is that the model is less complex than the one of random search, disproving my earlier claim of possible underfitting, with the test accuracy of this smaller model beating the test accuracy of the larger one it might have actually overfitted and I was simply fooled by it's circumstances, I will leave my earlier speculation in this notebook though so this reveal can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44459f",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "I will take my results in EX 1 as random search because it used an element of randomness, as to which configurations were tried.\n",
    "\n",
    "Due to a bug in the naming the droupout rate is not output in the end parameters (it was applied though in the model)\n",
    "\n",
    "Convergence speed: \n",
    "\n",
    "- random search: 45 mins\n",
    "- hyperband: 19 mins\n",
    "\n",
    "Best result: \n",
    "\n",
    "- **random search**\\\n",
    "    hidden_layers [64, 64, 64, 64]\\\n",
    "    dropout_rate 0.049\\\n",
    "    learning_rate 0.0021\\\n",
    "    activation_function elu\\\n",
    "    optimzer Adam\\\n",
    "    val acc 85.97%\n",
    "    test set acc: 79.74%\n",
    "- **hyperband**\\\n",
    "    hidden_layers [32, 32, 32, 32]\\\n",
    "    learning_rate: 0.0006\\\n",
    "    activation: celu'\\\n",
    "    optimizer lion\\\n",
    "    validation acc 86.32%\n",
    "    test set acc: 85.53%\n",
    "\n",
    "\n",
    "#### Better?\n",
    "\n",
    "No, such a approach is not **inherently** better than random search, while smarter and on **avarage probably yielding better** results (more exploitation), it is much easier to get stuck in local optima, while random search could simply get lucky and land in an even deeper optima (much more exploration)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
